# Supervised Learning

#### Classification

Some input `x` that maps to a label (e.g., a value belonging to a discrete set). For example `x` could be an image (e.g. photos of faces) and determining the gender of the person.


#### Regression

Some input `x` could map to a real value (e.g., a value that is part of a continual parameter) predicting the age of a person, based off of a photo of their face).

## Terminology

Multiple important terminologies exist that will be used throughout the class:

- ### Instances (Input)
Vectors of attributes that define the input space (e.g. pixel values of a picture, credit score examples containing income, )

- ### Concept 
A function that maps inputs to outputs (e.g., a picture that maps to true or false). References to the more general notion of a concept by describing what something is, or is not (e.g., a set of things that are ). A mapping between objects in the world and membership in a set.

- ### Target Concept
The actual answer we are searching for in the space of multiple concepts, also known as the *hypothesis class*. 

- ### Hypothesis (Class)
The set of all concepts you're trying to entertain. Could be all possible functions (however, given finite data, an infinite space to search).

- ### Sample (Training Set)
Examples of input output pairs (e.g., person profile and credit worthiness)

- Candidate
A *concept* that is believed to be the *target concept*.

- ### Testing Set
Structure is identical to the *training set*, however, **never** overlaps with examples belonging to the *training set*.

---

# Decision Trees


Example: Going on date with someone. 

Input: a list of restaurants with categorical labels (features):

- Type: Italian, French, Thai
- Atmosphere: Fancy, HIW, Casual
- Busy, 
- Status: High End
- Cost: could be discrete or an average number
- Weather (no)
- Date Situation: Hot, (or Not)

Output: True or False

## Presentation vs Algorithms
Decision trees are logical structures containing:

- **Nodes** represent attributes (hungry or not?)
- **Edges** represent values (yes, True or no, False)
- **Leaves** are end states, representing the output

Decision trees can be used to represent logic conditions such as OR or XOR.

- OR, (easy, any) when mapped out in a decision tree, demonstrates *linear* complexity in number of nodes (attributes). For $n$ number of nodes, there will be $n$ operations.

- XOR, (hard, parity) when mapped out in a decision tree, demonstrates *exponential* complexity. For $n$ nodes, there is a bound of $2^n -1$, $O(2^n)$.

### Parity

Even or odd parity define a boolean operation on a set of boolean data.

ODD: if number of attributes that are true is an odd number, the that the output is true, otherwise it's false.

EVEN: if number of attributes	

*Note*: In general in ML, we hope to encounter problems that are more like *any* vs *parity*.

## Expressiveness of Decision Trees

Decision Trees are extremeley expressive, allowing for an extremeley large hypothosis space. We must have clever ways to focus search using algorithims. 

## ID3 Algorithm

A top-down decision tree learning algorithm:

### Psuedocode

Loop:

 - $A$ $\leftarrow$ best attribute
 - Assign $A$ as decision attribute for *node*
 - For each value of $A$ create a descendent of *node*.
 - Sort training examples to *leaves*.
 - IF Examples perfectly classified $\rightarrow$ **STOP**
 - ELSE Iterate over leaves.
 
### Information Gain (e.g., finding the best attribute)
 
The amount of information gained by picking a particular attribute in Decision Tree making.
Mathmatically, information gain $IG$ quantifies the reduction in randomness over the labels you have with a set of data, based upon knowing the value of a particular attribute. It is defined as:
$$\textrm{IG}(S, A) = \textrm{H}(S) - \sum_v \frac{\left|S_v\right|}{\left|S\right|} \cdot \textrm{H}(S) $$ where: 

- $S$ is the set of training examples and
- $A$ is an attribute.
- $H$ is the entropy, defined as

$$  H(S) = - \sum_v p(v)\log(v) $$

## ID3 Bias 

*Restriction bias* describes the hypothosis set space $H$ we will consider for the learning effort (e.g., decision trees and not quatradic equations).

*Preference bias* desribes the subset of hypothoses, $n$, from the hypothosis set space that we *prefer* ($n \in H$), which is really at the heart of inductive bias.
 
### ID3's biases:
 
1. Good Splits at the top of the tree
2. Correct of Incorrect
3. Prefers shorter trees vs. longer (due to bias #1).

## Other Considerations

### Continuous Attributes (e.g., age, weight, distance)

With continuous attributes, it can make sense to repeat asking about an attribute along a path in a decision tree. For instances, consider age as an attribute, and the desire to continually refine the range an age belongs to by asking T/F questions about

### When do we stop?

- When everything is classified correctly?
- When we've run out of attributes?
- cross validation to prevent overfitting (a big complicated tree, in the case desicion trees).
- Breath-first search

- **Pruning**, Output: Vote

### Regression

- Splitting? Varience
- Output: Average, local linear fit

 